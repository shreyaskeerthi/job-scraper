# -*- coding: utf-8 -*-
"""jobscraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TawNZMvxoKMCXyBJjWrXLbtSrP9GHYvv
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import quote_plus, urlparse
import json
from datetime import datetime
import os

# param config
JOB_TITLE = "Data Scientist"
LOCATION = "New York City Metropolitan Area"
MAX_JOBS = 1000  # target
OUTPUT_CSV = "linkedin_jobs_complete.csv"
REQUEST_DELAY = (2, 5)
MAX_RETRIES = 5
TIMEOUT = 15
PROXY_LIST = []
SAVE_INTERVAL = 25


headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Referer': 'https://www.google.com/',
    'DNT': '1'
}

class LinkedInScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(headers)
        self.proxy_index = 0
        self.scraped_count = 0
        self.retry_count = 0
        self.jobs_data = []
        self.seen_links = set()

        # Load progress if output file exists
        if os.path.exists(OUTPUT_CSV):
            existing_df = pd.read_csv(OUTPUT_CSV)
            self.jobs_data = existing_df.to_dict('records')
            self.seen_links = set(existing_df['link'].tolist())
            self.scraped_count = len(self.jobs_data)
            print(f"Resuming from existing data with {self.scraped_count} jobs")

    def get_proxy(self):
        if not PROXY_LIST:
            return None
        proxy = PROXY_LIST[self.proxy_index % len(PROXY_LIST)]
        self.proxy_index += 1
        return {'http': proxy, 'https': proxy}

    def make_request(self, url, is_detail_page=False):
        proxies = self.get_proxy()
        retry_delay = 1

        for attempt in range(MAX_RETRIES):
            try:
                # Vary delay based on request type
                delay = random.uniform(
                    REQUEST_DELAY[0] * (1.5 if is_detail_page else 1),
                    REQUEST_DELAY[1] * (1.5 if is_detail_page else 1)
                )
                time.sleep(delay)

                response = self.session.get(
                    url,
                    proxies=proxies,
                    timeout=TIMEOUT,
                    headers={**headers, 'Referer': 'https://www.linkedin.com/jobs/'}
                )

                if response.status_code == 429:
                    retry_after = int(response.headers.get('Retry-After', random.randint(30, 120)))
                    print(f"Rate limited. Waiting {retry_after} seconds...")
                    time.sleep(retry_after)
                    continue

                response.raise_for_status()
                return response

            except requests.exceptions.RequestException as e:
                self.retry_count += 1
                if attempt < MAX_RETRIES - 1:
                    retry_delay *= 2  # Exponential backoff
                    wait_time = min(retry_delay + random.uniform(0, 3), 60)  # Cap at 60 seconds
                    print(f"Attempt {attempt + 1} failed ({e}). Retrying in {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    continue
                raise

        raise Exception(f"Failed after {MAX_RETRIES} attempts")

    def scrape_job_details(self, job_url):
        try:
            response = self.make_request(job_url, is_detail_page=True)
            job_soup = BeautifulSoup(response.text, "html.parser")

            # Extract description
            desc_div = job_soup.find("div", class_=lambda c: c and "show-more-less-html__markup" in c)
            description = desc_div.get_text(" ", strip=True) if desc_div else None

            # Extract salary (multiple approaches)
            salary = None
            salary_meta = job_soup.find("meta", attrs={"name": "salary"})
            if salary_meta:
                salary = salary_meta["content"]
            else:
                salary_div = job_soup.find("li", class_=lambda c: c and "salary" in c.lower())
                if salary_div:
                    salary = salary_div.get_text(" ", strip=True)

            # Extract job criteria
            criteria = {}
            for item in job_soup.select(".description__job-criteria-item"):
                key = item.select_one(".description__job-criteria-subheader").get_text(strip=True).lower().replace(" ", "_")
                value = item.select_one(".description__job-criteria-text").get_text(strip=True)
                criteria[key] = value

            return {
                "description": description,
                "salary": salary,
                **criteria
            }
        except Exception as e:
            print(f"Error scraping job details: {e}")
            return {}

    def save_progress(self):
        if not self.jobs_data:
            return

        temp_file = f"temp_{OUTPUT_CSV}"
        df = pd.DataFrame(self.jobs_data)

        # Ensure all expected columns exist
        expected_columns = ['title', 'company', 'location', 'posted', 'link',
                           'description', 'salary', 'seniority_level',
                           'employment_type', 'job_function', 'industries']

        for col in expected_columns:
            if col not in df.columns:
                df[col] = None

        df.to_csv(temp_file, index=False)
        os.replace(temp_file, OUTPUT_CSV)
        print(f"\nProgress saved. Total jobs collected: {len(df)}")

    def run(self):
        encoded_title = quote_plus(JOB_TITLE)
        encoded_location = quote_plus(LOCATION)
        start_time = datetime.now()

        print(f"\nStarting LinkedIn job search for '{JOB_TITLE}' in '{LOCATION}' at {start_time}")
        print(f"Target: {MAX_JOBS} jobs | Current: {self.scraped_count} jobs")

        start_param = 0
        consecutive_empty = 0

        while self.scraped_count < MAX_JOBS and consecutive_empty < 5:
            search_url = f"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={encoded_title}&location={encoded_location}&start={start_param}"

            try:
                print(f"\nFetching batch starting at {start_param}...")
                response = self.make_request(search_url)

                soup = BeautifulSoup(response.text, "html.parser")
                jobs = soup.find_all("li")

                if not jobs:
                    consecutive_empty += 1
                    print(f"No jobs found in batch. Empty counter: {consecutive_empty}/5")
                    start_param += 25
                    continue

                consecutive_empty = 0
                print(f"Processing {len(jobs)} jobs in this batch")

                for job in jobs:
                    try:
                        if self.scraped_count >= MAX_JOBS:
                            break

                        link_tag = job.find("a", class_="base-card__full-link")
                        job_url = link_tag["href"].split("?")[0] if link_tag else None

                        if not job_url or job_url in self.seen_links:
                            continue

                        self.seen_links.add(job_url)

                        title = job.find("h3", class_="base-search-card__title").get_text(strip=True)
                        company = job.find("h4", class_="base-search-card__subtitle").get_text(strip=True)
                        location = job.find("span", class_="job-search-card__location").get_text(strip=True)
                        posted = job.find("time")["datetime"] if job.find("time") else None

                        job_info = {
                            "title": title,
                            "company": company,
                            "location": location,
                            "posted": posted,
                            "link": job_url
                        }

                        # Get detailed info
                        details = self.scrape_job_details(job_url)
                        job_info.update(details)

                        self.jobs_data.append(job_info)
                        self.scraped_count += 1

                        print(f"  {self.scraped_count}. {title} at {company}")

                        # Periodic save
                        if self.scraped_count % SAVE_INTERVAL == 0:
                            self.save_progress()

                    except Exception as e:
                        print(f"Error processing job: {e}")
                        continue

                start_param += 25

            except Exception as e:
                print(f"Fatal error in batch processing: {e}")
                break

        # Final save
        self.save_progress()

        duration = datetime.now() - start_time
        print(f"\nScraping completed in {duration}")
        print(f"Total jobs collected: {self.scraped_count}")
        print(f"Retry attempts: {self.retry_count}")
        print(f"Results saved to: {OUTPUT_CSV}")

if __name__ == "__main__":
    scraper = LinkedInScraper()
    scraper.run()